\startcomponent *

\product  prd-analysis


\startchapter [title={Classification of stochastic processes}]
	This is well written in Cosma Rohilla Shalizi - Almost None of the Theory of Stochastic (2010), Chapter 1. Let \m{X} be a stochastic process given by
	\startformula \startalign[n=3, align={right,right,left}]
		\NC  X :  \NC  ğ•‹  Ã—   Î©  \NC  â†’  Î  \NR
		\NC       \NC  \qquad â„±  \NC  â†’  ğ“§  \NR
		\NC       \NC  (t,    Ï‰)  \NC  â†¦  X(t, Ï‰) .  \NR
	\stopalign \stopformula

	The spaces are as follows.

	\dscr{\m{ğ•‹}}  The \emph{index set}. Can be finite, discrete (countable) or continuous (uncountable). Can be one-sided, two-sided, spatially distributed, or sets.

	\dscr{\m{(Î, ğ“§)}}  The \emph{state space}. Requirements: measurable. Can be finite, discrete or continuous.

	\dscr{\m{(Î©, â„±, â„™)}}  The \emph{probability space}.

	\startitemize [1, nowhite, after]
		\item  If \m{ğ•‹ = \bcrl[1], Î = â„}, then \m{X} is a \emph{random variable}.
		\item  If \m{ğ•‹ = \bcrl[1, â€¦, n], Î = â„}, then \m{X} is a \emph{random vector}.
		\item  If \m{ğ•‹ = \bcrl[1], Î = â„^d}, then \m{X} is a \emph{random vector}.
		\item  If \m{ğ•‹ = â„•, Î = â„}, then \m{X} is a \emph{one-sided random sequence} or \emph{one-sided discrete-time stochastic process}.
		\item  If \m{ğ•‹ = â„¤, Î = â„}, then \m{X} is a \emph{two-sided random variable} or \emph{two-sided discrete-time stochastic process}.
		\item  If \m{ğ•‹ = â„¤^d, Î = â„}, then \m{X} is a \emph{spatial random variable}.
		\item  If \m{ğ•‹ = â„, Î = â„}, then \m{X} is a \emph{continuous-time random variable}.
		\item  If \m{ğ•‹ = ğ“‘, Î = [0, âˆ]}, then \m{X} is a \emph{random set function on the reals}.
		\item  If \m{ğ•‹ = ğ“‘ Ã— â„•, Î = [0, âˆ]}, then \m{X} is a \emph{one-sided random sequence of set function on the reals}.
		\item  \emph{Emperical measures.}  Let \m{(Z_n)} be an i.i.d. random sequence and define \m{\hat{â„™}_n : ğ“‘ Ã— Î©  \NC  â†’  ğ“Ÿ: (B, Ï‰)  â†¦  \hat{â„™}_n(B, Ï‰)  =  \frac{1}{n} âˆ‘_{j = 1}^n ğŸ™_B(Z_j(Ï‰))}. Then \m{\hat{â„™}_n} is a \emph{one-sided random sequence of set function on the reals}, which are in fact \emph{probability measures}. \comment{[\m{ğ“Ÿ} is the space of probability measures on \m{â„}.]}
		\item  If \m{ğ•‹ = ğ“‘^d, Î = [0, âˆ]}, then \m{X} is the class of set functions on \m{â„^d}. Let \m{ğ“œ} be the subclass of measures. Then a random set function with realizations in \m{ğ“œ} is called a \emph{random measure}.
		\item  If \m{ğ•‹ = ğ“‘^d, \abs[Î] < âˆ}, then \m{X} is a \emph{point process}.
		\item  If \m{ğ•‹ = [0, âˆ), Î = â„^d < âˆ}. A \m{Î}-valued random process on \m{ğ•‹} with paths in \m{C(ğ•‹)} is a \emph{continuous random process}. E.g. Wiener process.
	\stopitemize
\stopchapter

\startchapter [title={Martingales}, reference=sub:martingales]
	\startsection [title={New martingales from old}]
		A stochastic process \m{A = (A_n)} is called adapted if \m{âˆ€ n âˆˆ â„•, A_n âˆˆ L^0(â„±_n)}. Let \m{M = (M_n)} be a martingale. Then process \m{\tilde{M} = (\tilde{M}_n)} defined by \m{(A â‹… M)_n = \tilde{M}_n = âˆ‘_{j = 0}^{n - 1} A_j Î”M_j}, where \m{Î”M_j = M_{j + 1} - M_j}, is called the \emph{martingale transform} of \m{M} by \m{A}.

		Theorem (martingale transform theorem): \m{\tilde{M}} is a martingale.

		Proof.
		\startformula
			ğ”¼(Î” \tilde{M}_n âˆ£ â„±_n)  =  ğ”¼(A_n Î”M_n âˆ£ â„±_n)  =  A_n ğ”¼(Î”M_n âˆ£ â„±_n)  =  0 .
		\stopformula

		Now, let \m{X_n} be a stochastic process and \m{Ï„} be a stopping time. Define the stopped process \m{X_Ï„ = âˆ‘_{j = 0}^âˆ ğŸ™_{\bcrl[Ï„ = j]} X_j} when \m{â„™(Ï„ < âˆ) = 1}.

		Theorem (stopping time theorem): Let \m{(M_n)} be a martingale with respect to \m{(â„±_n)}. Then \m{(M_{n âˆ§ Ï„})} is also a martingale with respect to \m{(â„±_n)}.

		Proof. Without loss of generality, assume \m{M_0 = 0}, otherwise we can translate by \m{M_0} as \m{\tilde{M_n} = M_n - M_0}. Now, the \emph{stake process} \m{A_n = ğŸ™_{\bcrl[Ï„ > n]} = 1 - ğŸ™_{\bcrl[Ï„ â©½ n]}} is adapted to \m{(â„±_n)} and is bounded by \m{n}. Now,
		\startformula \startalign
			\NC  (A â‹… M)_n  \NC =  âˆ‘_{j = 0}^{n - 1} A_j Î”M_j  \NR
			\NC  \NC =  âˆ‘_{j = 0}^{n - 1} Î”M_j - âˆ‘_{j = 0}^{n - 1} ğŸ™_{\bcrl[Ï„ â©½ j]} \brnd[M_{j + 1} - M_j]  \NR
			\NC  \NC =  M_n - M_0 - M_n ğŸ™_{\bcrl[Ï„ â©½ n]} + âˆ‘_{j = 0}^{n - 1} \brnd[ğŸ™_{\bcrl[Ï„ â©½ j]} M_j - ğŸ™_{\bcrl[Ï„ â©½ {j - 1}]} M_j]  \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + âˆ‘_{j = 0}^{n - 1} M_j ğŸ™_{\bcrl[Ï„ = j]}   \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + âˆ‘_{j = 0}^{n - 1} M_Ï„ ğŸ™_{\bcrl[Ï„ = j]}   \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + M_Ï„ âˆ‘_{j = 0}^{n - 1} ğŸ™_{\bcrl[Ï„ = j]}   \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + M_Ï„ ğŸ™_{\bcrl[Ï„ â©½ n]}   \NR
			\NC  \NC =  M_{n âˆ§ Ï„} .  \NR
		\stopalign \stopformula
		Therefore, \m{(M_{n âˆ§ Ï„})} is a martingale transform of \m{(M_n)}. Since \m{(A_n)} is bounded and adapted, by the martingale transform theorem, \m{(M_{n âˆ§ Ï„})} is a martingale.

	\stopsection

\stopchapter


\startchapter [title={ItÃ´ calculus}, reference=sub:ItÃ´]
	Notation:

	In what follows, \m{T = [0, âˆ)}, \m{ğ“} means adapted, \m{ğ“‘} means bounded, \m{ğ“’} means continuous, and \m{\norm[â‹…]} denotes the \m{L^2}-norm.

	Let \m{(Î©, â„±, ğ”½ = (â„±_t), â„™)} be a filtered probability space, \m{W:T Ã— Î© â†’ â„‚} be a \m{ğ”½}-adapted Wiener martingale, and \m{X: T Ã— Î© â†’ â„‚} be a stochastic process.

	\startsection [title={Step 1: \m{X âˆˆ ğ“ âˆ© ğ“¢} a.s.}]
		Let \m{X(t, Ï‰) = âˆ‘_{j â‰¥ 0} Î¾_j(Ï‰) ğŸ™_{[t_j, t_{j+1})}(t)}, where \m{Î¾_j âˆˆ L^0(â„±_{t_j})}.
	\stopsection
	\startsection [title={Step 2: \m{X âˆˆ ğ“ âˆ© ğ“‘ âˆ© ğ“’} a.s.}]
		Define \m{X_n(t, Ï‰) = X\brnd[\frac{\floor[nt]}{n}, Ï‰], \ n âˆˆ â„•}. Note that \m{âˆ€n, X_n âˆˆ ğ“ âˆ© ğ“¢}, and since \m{X âˆˆ ğ“’}, \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)] â†’ 0} (pointwise convergence) \m{(t, Ï‰)}-a.s. Then \m{âˆ€ Îµ > 0}, there exists a sufficiently large \m{n âˆˆ â„•} such that \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)] < Îµ < âˆ} (bounded) \m{(t, Ï‰)}-a.s, so \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 < Îµ^2 < âˆ} \m{(t, Ï‰)}-a.s. Therefore, by the bounded convergence theorem, \m{\norm[X_n - X] â†’ 0}.

		Therefore, \m{(X_n)} is Cauchy in \m{L^2(T Ã— Î©)}, that is, \m{\norm[X_n - X_m] â†’ 0}. Now, by linearity and ItÃ´ isometry for the ItÃ´ integral for simple processes, \m{\norm[ğ“˜(X_n) - ğ“˜(X_m)] = \norm[ğ“˜(X_n - X_m)] = \norm[X_n - X_m] â†’ 0}. Therefore, for \m{t âˆˆ T} fixed, \m{(ğ“˜(X_n))} is Cauchy in \m{L^2(Î©)}. Since \m{L^2(Î©)} is complete, the sequence converges. Denote the limit by \m{ğ“˜(X)}, that is, \m{\norm[ğ“˜(X_n) - ğ“˜(X)] â†’ 0}.

	\stopsection
	\startsection [title={Step 3: \m{X âˆˆ ğ“ âˆ© ğ“‘ âˆ© L^0(T Ã— Î©)}}]

	\stopsection
	\startsection [title={Step 4: \m{X âˆˆ ğ“ âˆ© L^2(T Ã— Î©)}}]

	\stopsection
	\startsection [title={Step 5: \m{X âˆˆ ğ“ âˆ© \bcrl[X âˆˆ â„‚^{T Ã— Î©} : âˆ€ t â‰¥ 0, âˆ«_0^t X(s, â‹…) \d s < âˆ] a.s.}}]

	\stopsection
	\startsection [title={Properties of the ItÃ´ integral}]
		In what follows, assume the following. Let \m{X, Y âˆˆ ğ“ âˆ© L^2(T Ã— Î©); (X_n), (Y_n) âŠ‚ ğ“ âˆ© ğ“¢} such that \m{\norm[X_n - X] â†’ 0} and \m{\norm[Y_n - Y] â†’ 0}. Let \m{z âˆˆ â„‚}.

		\startsubsection [title={Linearity: \m{\norm[z ğ“˜(X) + ğ“˜(Y) - ğ“˜(zX+Y)] = 0}}]
			First, note that \m{\norm[(z X_n + Y_n) - (z X + Y)] â‰¤ \abs[z] \norm[X_n - X] + \norm[Y_n - Y] â†’ 0}. Now, by the linearity of the integral \m{ğ“˜:ğ“ âˆ© ğ“¢ â†’ L^2(Î©)}, we have
			\startformula \startalign[align={left}]
				\NC  \norm[z ğ“˜(X) + ğ“˜(Y) - ğ“˜(z X + Y)]  \NR
				\NC  =  \norm[z ğ“˜(X) + ğ“˜(Y) - z ğ“˜(X_n) - ğ“˜(Y_n) + ğ“˜(z X_n + Y_n) - ğ“˜(z X + Y)]  \NR
				\NC  â‰¤  \abs[z] \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[ğ“˜(Y) - ğ“˜(Y_n)] + \norm[ğ“˜(z X_n + Y_n) - ğ“˜(z X + Y)]  â†’  0 .
			\stopalign \stopformula
		\stopsubsection
		\startsubsection [title={ItÃ´ isometry: \m{\norm[ğ“˜(X)] = \norm[X]}}]
			Using the isometry of the integral \m{ğ“˜:ğ“ âˆ© ğ“¢ â†’ L^2(Î©)}, we have
			\startformula \startalign[align={left,left}]
				\NC  \norm[ğ“˜(X)]  \NC  â‰¤  \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[ğ“˜(X_n)]  \NR
				\NC  \NC  =  \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[X_n]  \NR
				\NC  \NC  =  \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[X_n - X] + \norm[X]  â†’  \norm[X] .
			\stopalign \stopformula
			Note that the \quote{ItÃ´ isometry} is actually a unitary transformation.
		\stopsubsection
		\startsubsection [title={Martingale property: \m{ğ”¼\brnd[ğ“˜_t(X) âˆ£ â„±_s] = ğ“˜_s(X)} a.s.}]
			The martingale property of the integral \m{ğ“˜:ğ“ âˆ© ğ“¢ â†’ L^2(Î©)} gives \m{ğ”¼\brnd[ğ“˜_t(X_n) - ğ“˜_s(X_n) âˆ£ â„±_s] = 0}. Using this and the unitariness of the ItÃ´ isometry, we get
			\startformula \startalign[align={left,left}]
				\NC  \norm[ğ”¼{\brnd[ğ“˜_t(X) - ğ“˜_s(X) âˆ£ â„±_s]}]^2  \NR
				\NC  =  ğ”¼\abs[ğ”¼{\brnd[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X) âˆ£ â„±_s]} + ğ”¼{\brnd[ğ“˜_t(X_n) - ğ“˜_s(X_n) âˆ£ â„±_s]}]^2  \NR
				\NC  =  ğ”¼\abs[ğ”¼{\brnd[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X) âˆ£ â„±_s]} + 0]^2  \NR
				\NC  â‰¤  ğ”¼ğ”¼\brnd[{\abs[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X)]}^2 âˆ£ â„±_s]  \NR
				\NC  =  ğ”¼\abs[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X)]^2  \NR
				\NC  =  \norm[ğ“˜_t(X - X_n) + ğ“˜_s(X_n - X)]^2  \NR
				\NC  â‰¤  2 \brnd[{\norm[ğ“˜_t(X - X_n)]}^2 + {\norm[ğ“˜_s(X_n - X)]}^2]  \qquad  \bsqr[{\norm[a + b]^2 â‰¤ 2\brnd[{\norm[a]^2 + \norm[b]^2}]}]  \NR
				\NC  =  2 \brnd[{\norm[X - X_n]}^2 + {\norm[X_n - X]}^2]  =  4 \norm[X_n - X]^2  â†’  0 .
			\stopalign \stopformula
		\stopsubsection
	\stopsection

	% In what follows, \m{T = [0, t], \ t âˆˆ [0, âˆ)}, \m{X: T Ã— Î© â†’ â„}, and \m{ğ“¢} represent the class of simple processes.

	% \startsection [title={Approximation of \m{ğ“¢ âˆ‹ X_n â†’ X âˆˆ L^2(T Ã— Î©)}}]
	% 	\startsubsection [title={Step 1: \m{ğ“¢ âˆ‹ X_n â†’ X âˆˆ C âˆ© B}}]
	% 		Suppose \m{X} is a progressive, adapted, \emph{bounded on \m{I}} \m{t}-a.s., and has \emph{continuous sample paths} \m{t}-a.s. Then there exist a sequence of bounded simple processes \m{(X_n)} such that \m{X_n â†’ X} in \m{L^2(T Ã— Î©)}.

	% 		Proof.

	% 		Let \m{(Î”_n)} be a sequence of progressively finer partitions of \m{I} such that \m{Î”_n = \bcrl[t_0, t_1 â€¦, t_n]; t_0 < t_1 < â€¦ < t_n}, and \m{\norm[Î”_n] â†’ 0} as \m{n â†’ âˆ}. Define the sequence of simple processes \m{X_n(t, Ï‰) = âˆ‘_{j = 0}^{n - 1} X(t_j, Ï‰) ğŸ™_{\intco[t_j, t_{j + 1}]}(t)}. Then, we claim that \m{X_n â†’ X} in \m{L^2(T Ã— Î©)}.

	% 		Firstly fix \m{Ï‰ âˆˆ Î©}. By the \emph{continuity of paths} of \m{X}, \m{âˆ€ Ïµ > 0, âˆƒ Î´ > 0} such that whenever \m{\abs[s - t] < Î´}, we have \m{\abs[X(s, Ï‰) - X(t, Ï‰)] < Ïµ}. Therefore, for \m{\norm[Î”_n] < Î´}, we have
	% 		\startformula
	% 			\abs[X_n(t, Ï‰) - X(t, Ï‰)]
	% 				â‰¤ âˆ‘_{j = 0}^{n - 1} \abs[X(t_j, Ï‰) - X(t, Ï‰)] ğŸ™_{\intco[t_j, t_{j + 1}]}(t)
	% 				< âˆ‘_{j = 0}^{n - 1} Ïµ ğŸ™_{\intco[t_j, t_{j + 1}]}(t)
	% 				= Ïµ .
	% 		\stopformula
	% 		Therefore, \m{âˆ€ t âˆˆ I}, \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)] â†’ 0}, and so \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 â†’ 0}. Moreover, since the sample path \m{X(â‹…, Ï‰)} is \emph{bounded}, we have \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 â©½ 4 \norm[X(â‹…, Ï‰)]_{L^âˆ(I)}^2 < âˆ}. Therefore by the bounded convergence theorem, \m{âˆ«_0^1 \abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 \d t â†’ 0}.

	% 		Now varying \m{Ï‰} and taking expectation, we get \m{ğ”¼\brnd[âˆ«_0^1 {\abs[X_n(t, Ï‰) - X(t, Ï‰)]}^2 \d t] â†’ 0}. In other words, \m{\norm[X_n - X]_{L^2(T Ã— Î©)} â†’ 0}.
	% 	\stopsubsection
	% 	\startsubsection [title={Step 2: \m{C âˆ© B âˆ‹ X_n â†’ X âˆˆ B}}]

	% 	\stopsubsection
	% 	\startsubsection [title={Step 3: \m{B âˆ‹ X_n â†’ X âˆˆ L^2(T Ã— Î©)}}]

	% 	\stopsubsection
	% \stopsection

\stopchapter


\startchapter [title={Examples}]
	\startsection [title={Find \m{ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t]^2}.}]
		By Fubini's theorem,
		\startformula
			ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t]^2  \NC  =  ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t âˆ«_0^1 B_s^2 \d s]  =  ğ”¼\brnd[âˆ«_0^1 âˆ«_0^1 B_t^2 B_s^2 \d s \d t]  =  âˆ«_0^1 âˆ«_0^1 ğ”¼(B_t^2 B_s^2) \d s \d t .
		\stopformula
		\startformula \startalign[n=3, align={left, right, left}]
			\NC  \text{Now, } âˆ€s âˆˆ [0, t],  \NC  ğ”¼(B_t^2 B_s^2)  \NC  =  ğ”¼(ğ”¼(B_t^2 B_s^2 âˆ£ â„±_s))  =  ğ”¼(B_s^2 ğ”¼((B_t^2 - t) + t âˆ£ â„±_s))  \NR
			\NC  \NC  \NC  =  ğ”¼(B_s^2 ((B_s^2 - s) + t))  =  ğ”¼(B_s^2 ((B_s^2 - s) + t))  \NR
			\NC  \NC  \NC  =  ğ”¼(B_s^4 - s B_s^2 + t B_s^2)  =  3s^2 - s^2 + ts  =  2s^2 + ts .  \NR
			\NC  \text{So }  \NC  ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t]^2  \NC  =  2 âˆ«_0^1 âˆ«_0^t (2s^2 + ts) \d s \d t  =  \frac{7}{9} .  \NR
		\stopalign \stopformula
	\stopsection
	\startsection [title={Find \m{ğ•\brnd[âˆ«_0^1 t^2 B_t \d t]}.}]
		By Fubini's theorem, \m{ğ”¼\brnd[âˆ«_0^1 t^2 B_t \d t]  =  âˆ«_0^1 t^2 ğ”¼B_t \d t  =  0}. So by Fubini's theorem (again),
		\startformula \startalign[align={left, left}]
			\NC  ğ•\brnd[âˆ«_0^1 t^2 B_t \d t]  \NC  =  ğ”¼\brnd[âˆ«_0^1 t^2 B_t \d t]^2  =  ğ”¼\brnd[âˆ«_0^1 t^2 B_t \d t âˆ«_0^1 s^2 B_s \d s]  \NR
			\NC  \NC  =  ğ”¼\brnd[âˆ«_0^1 âˆ«_0^1 t^2 s^2 B_t B_s \d s \d t]  =  âˆ«_0^1 âˆ«_0^1 t^2 s^2 ğ”¼(B_t B_s) \d s \d t  \NR
			\NC  \NC  =  âˆ«_0^1 âˆ«_0^1 t^2 s^2 (t âˆ§ s) \d s \d t  =  2 âˆ«_0^1 âˆ«_0^t t^2 s^2 s \d s \d t  =  \frac{1}{14} .  \NR
		\stopalign \stopformula
	\stopsection
\stopchapter

\stopcomponent
