\startcomponent *

\product  prd-analysis


\startchapter [title={Brownian motion}]
	
	In the following, let
	\startitemize [5, joinedup]
		\item  \m{Î”_i t = t_i - t_{i-1}} with \m{Î”_1 t = t_1},
		\item  \m{Î”_i x = x_i - x_{i-1}}, with  and \m{Î”_1 x = x_1},
		\item  \m{Î”_i X = X_{t_i} - X_{t_{i-1}}}, with  and \m{Î”_1 X = X_{t_1}}.
	\stopitemize

	\starttheorem
		The following are equivalent
		\startitemize [m, joineup]
			\item[bm-increment]  \m{X_t} is a stochastic process having independent Gaussian increments.
			\item[bm-marginal]  \m{X_t} is a stochastic process with marginal distributions given by
				\startformula
					Î¼_{t_1, â€¦, t_n}(A) = \frac{1}{(2Ï€)^{\frac{n}{2}} âˆ Î”_i t} âˆ«_A \exp \brnd{-\frac12 âˆ‘ \frac{(Î”_i x)^2}{Î”_i t}} âˆ \d x_i
				\stopformula
				for any \m{0 < t_1 < â‹¯ < t_n} and \m{n âˆˆ â„•}.
			\item[bm-Fourier]  \m{X_t} is a stochastic process such that for any \m{0 < t_1 < â‹¯ < t_n} and \m{Î»_1, â‹¯, Î»_n âˆˆ â„},
				\startformula
					ğ”¼ \exp \brnd{i âˆ‘ Î»_i Î”_i X_i} = \exp\brnd{-\frac12 âˆ‘ Î»_i^2 Î”_i t} .
				\stopformula
		\stopitemize
	\stoptheorem

	\startproof
		\startitemize [i]
			\item  \in[bm-increment] âŸ¹ \in[bm-marginal]

				TODO
		\stopitemize
	\stopproof
\stopchapter


\startchapter [title={Classification of stochastic processes}]

	This is well written in Cosma Rohilla Shalizi - Almost None of the Theory of Stochastic (2010), Chapter 1. Let \m{X} be a stochastic process given by
	\startformula \startalign[n=3, align={right,right,left}]
		\NC  X :  \NC  ğ•‹  Ã—   Î©  \NC  â†’  Î  \NR
		\NC       \NC  \qquad â„±  \NC  â†’  ğ“§  \NR
		\NC       \NC  (t,    Ï‰)  \NC  â†¦  X(t, Ï‰) .  \NR
	\stopalign \stopformula

	The spaces are as follows.

	\dscr{\m{ğ•‹}}  The \emph{index set}. Can be finite, discrete (countable) or continuous (uncountable). Can be one-sided, two-sided, spatially distributed, or sets.

	\dscr{\m{(Î, ğ“§)}}  The \emph{state space}. Requirements: measurable. Can be finite, discrete or continuous.

	\dscr{\m{(Î©, â„±, â„™)}}  The \emph{probability space}.

	\startitemize [1, nowhite, after]
		\item  If \m{ğ•‹ = \bcrl[1], Î = â„}, then \m{X} is a \emph{random variable}.
		\item  If \m{ğ•‹ = \bcrl[1, â€¦, n], Î = â„}, then \m{X} is a \emph{random vector}.
		\item  If \m{ğ•‹ = \bcrl[1], Î = â„^d}, then \m{X} is a \emph{random vector}.
		\item  If \m{ğ•‹ = â„•, Î = â„}, then \m{X} is a \emph{one-sided random sequence} or \emph{one-sided discrete-time stochastic process}.
		\item  If \m{ğ•‹ = â„¤, Î = â„}, then \m{X} is a \emph{two-sided random variable} or \emph{two-sided discrete-time stochastic process}.
		\item  If \m{ğ•‹ = â„¤^d, Î = â„}, then \m{X} is a \emph{spatial random variable}.
		\item  If \m{ğ•‹ = â„, Î = â„}, then \m{X} is a \emph{continuous-time random variable}.
		\item  If \m{ğ•‹ = \mathcal{B}, Î = [0, âˆ]}, then \m{X} is a \emph{random set function on the reals}.
		\item  If \m{ğ•‹ = \mathcal{B} Ã— â„•, Î = [0, âˆ]}, then \m{X} is a \emph{one-sided random sequence of set function on the reals}.
		\item  \emph{Emperical measures.}  Let \m{(Z_n)} be an i.i.d. random sequence and define \m{\hat{â„™}_n : \mathcal{B} Ã— Î©  \NC  â†’  ğ“Ÿ: (B, Ï‰)  â†¦  \hat{â„™}_n(B, Ï‰)  =  \frac{1}{n} âˆ‘_{j = 1}^n ğŸ™_B(Z_j(Ï‰))}. Then \m{\hat{â„™}_n} is a \emph{one-sided random sequence of set function on the reals}, which are in fact \emph{probability measures}. \comment{[\m{ğ“Ÿ} is the space of probability measures on \m{â„}.]}
		\item  If \m{ğ•‹ = \mathcal{B}^d, Î = [0, âˆ]}, then \m{X} is the class of set functions on \m{â„^d}. Let \m{ğ“œ} be the subclass of measures. Then a random set function with realizations in \m{ğ“œ} is called a \emph{random measure}.
		\item  If \m{ğ•‹ = \mathcal{B}^d, \abs[Î] < âˆ}, then \m{X} is a \emph{point process}.
		\item  If \m{ğ•‹ = [0, âˆ), Î = â„^d < âˆ}. A \m{Î}-valued random process on \m{ğ•‹} with paths in \m{C(ğ•‹)} is a \emph{continuous random process}. E.g. Wiener process.
	\stopitemize

\stopchapter


\startchapter [title={Martingales}, reference=sub:martingales]

	\startsection [title={New martingales from old}]

		A stochastic process \m{A = (A_n)} is called adapted if \m{âˆ€ n âˆˆ â„•, A_n âˆˆ L^0(â„±_n)}. Let \m{M = (M_n)} be a martingale. Then process \m{\tilde{M} = (\tilde{M}_n)} defined by \m{(A â‹… M)_n = \tilde{M}_n = âˆ‘_{j = 0}^{n - 1} A_j Î”M_j}, where \m{Î”M_j = M_{j + 1} - M_j}, is called the \emph{martingale transform} of \m{M} by \m{A}.

		Theorem (martingale transform theorem): \m{\tilde{M}} is a martingale.

		Proof.
		\startformula
			ğ”¼(Î” \tilde{M}_n âˆ£ â„±_n)  =  ğ”¼(A_n Î”M_n âˆ£ â„±_n)  =  A_n ğ”¼(Î”M_n âˆ£ â„±_n)  =  0 .
		\stopformula

		Now, let \m{X_n} be a stochastic process and \m{Ï„} be a stopping time. Define the stopped process \m{X_Ï„ = âˆ‘_{j = 0}^âˆ ğŸ™_{\bcrl[Ï„ = j]} X_j} when \m{â„™(Ï„ < âˆ) = 1}.

		Theorem (stopping time theorem): Let \m{(M_n)} be a martingale with respect to \m{(â„±_n)}. Then \m{(M_{n âˆ§ Ï„})} is also a martingale with respect to \m{(â„±_n)}.

		Proof. Without loss of generality, assume \m{M_0 = 0}, otherwise we can translate by \m{M_0} as \m{\tilde{M_n} = M_n - M_0}. Now, the \emph{stake process} \m{A_n = ğŸ™_{\bcrl[Ï„ > n]} = 1 - ğŸ™_{\bcrl[Ï„ â©½ n]}} is adapted to \m{(â„±_n)} and is bounded by \m{n}. Now,
		\startformula \startalign
			\NC  (A â‹… M)_n  \NC =  âˆ‘_{j = 0}^{n - 1} A_j Î”M_j  \NR
			\NC  \NC =  âˆ‘_{j = 0}^{n - 1} Î”M_j - âˆ‘_{j = 0}^{n - 1} ğŸ™_{\bcrl[Ï„ â©½ j]} \brnd[M_{j + 1} - M_j]  \NR
			\NC  \NC =  M_n - M_0 - M_n ğŸ™_{\bcrl[Ï„ â©½ n]} + âˆ‘_{j = 0}^{n - 1} \brnd[ğŸ™_{\bcrl[Ï„ â©½ j]} M_j - ğŸ™_{\bcrl[Ï„ â©½ {j - 1}]} M_j]  \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + âˆ‘_{j = 0}^{n - 1} M_j ğŸ™_{\bcrl[Ï„ = j]}   \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + âˆ‘_{j = 0}^{n - 1} M_Ï„ ğŸ™_{\bcrl[Ï„ = j]}   \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + M_Ï„ âˆ‘_{j = 0}^{n - 1} ğŸ™_{\bcrl[Ï„ = j]}   \NR
			\NC  \NC =  M_n ğŸ™_{\bcrl[Ï„ > n]} + M_Ï„ ğŸ™_{\bcrl[Ï„ â©½ n]}   \NR
			\NC  \NC =  M_{n âˆ§ Ï„} .  \NR
		\stopalign \stopformula
		Therefore, \m{(M_{n âˆ§ Ï„})} is a martingale transform of \m{(M_n)}. Since \m{(A_n)} is bounded and adapted, by the martingale transform theorem, \m{(M_{n âˆ§ Ï„})} is a martingale.

	\stopsection

\stopchapter


\startchapter[title={Markov processes}, reference=sub:Markov]

	\startsubject[title={Equivalent definitions}]
		Let \m{s âˆˆ [0, t]}. Then \m{X_â‹…} is a Markov process if any of the following are true:
		\startitemize[1, nowhite, after]
			\item  \m{âˆ€ E âˆˆ â„±, â„™(X_t âˆˆ E âˆ£ â„±_s) = â„™(X_t âˆˆ E âˆ£ X_s)}, or
			\item  \m{âˆ€ E âˆˆ â„±, âˆ€ f âˆˆ L^0 âˆ© \mathcal{B}, ğ”¼(f(X_t) âˆ£ â„±_s) = ğ”¼(f(X_t) âˆ£ X_s)}.
		\stopitemize
	\stopsubject

	\startsubject[title={Martingale vs Markov}]
		See \goto{djalil.chafai.net}[url(http://djalil.chafai.net/blog/2012/01/20/martingales-which-are-not-markov-chains/)] and \goto{MathSx:763645}[url(https://math.stackexchange.com/questions/763645)].
	\stopsubject

\stopchapter


\startchapter [title={ItÃ´ calculus}, reference=sub:ItÃ´]

	Notation:

	In what follows, \m{T = [0, âˆ)}, \m{\mathcal{A}} means adapted, \m{\mathcal{B}} means bounded, \m{\mathcal{C}} means continuous, and \m{\norm[â‹…]} denotes the \m{L^2}-norm.

	Let \m{(Î©, â„±, ğ”½ = (â„±_t), â„™)} be a filtered probability space, \m{W:T Ã— Î© â†’ â„‚} be a \m{ğ”½}-adapted Wiener martingale, and \m{X: T Ã— Î© â†’ â„‚} be a stochastic process.

	\startsection [title={Step 1: \m{X âˆˆ \mathcal{A} âˆ© \mathcal{S}} a.s.}]

		Let \m{X(t, Ï‰) = âˆ‘_{j â‰¥ 0} Î¾_j(Ï‰) ğŸ™_{[t_j, t_{j+1})}(t)}, where \m{Î¾_j âˆˆ L^0(â„±_{t_j})}.

	\stopsection

	\startsection [title={Step 2: \m{X âˆˆ \mathcal{A} âˆ© \mathcal{B} âˆ© \mathcal{C}} a.s.}]

		Define \m{X_n(t, Ï‰) = X\brnd[\frac{\floor[nt]}{n}, Ï‰], \ n âˆˆ â„•}. Note that \m{âˆ€n, X_n âˆˆ \mathcal{A} âˆ© \mathcal{S}}, and since \m{X âˆˆ \mathcal{C}}, \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)] â†’ 0} (pointwise convergence) \m{(t, Ï‰)}-a.s. Then \m{âˆ€ Îµ > 0}, there exists a sufficiently large \m{n âˆˆ â„•} such that \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)] < Îµ < âˆ} (bounded) \m{(t, Ï‰)}-a.s, so \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 < Îµ^2 < âˆ} \m{(t, Ï‰)}-a.s. Therefore, by the bounded convergence theorem, \m{\norm[X_n - X] â†’ 0}.

		Therefore, \m{(X_n)} is Cauchy in \m{L^2(T Ã— Î©)}, that is, \m{\norm[X_n - X_m] â†’ 0}. Now, by linearity and ItÃ´ isometry for the ItÃ´ integral for simple processes, \m{\norm[ğ“˜(X_n) - ğ“˜(X_m)] = \norm[ğ“˜(X_n - X_m)] = \norm[X_n - X_m] â†’ 0}. Therefore, for \m{t âˆˆ T} fixed, \m{(ğ“˜(X_n))} is Cauchy in \m{L^2(Î©)}. Since \m{L^2(Î©)} is complete, the sequence converges. Denote the limit by \m{ğ“˜(X)}, that is, \m{\norm[ğ“˜(X_n) - ğ“˜(X)] â†’ 0}.

	\stopsection

	\startsection [title={Step 3: \m{X âˆˆ \mathcal{A} âˆ© \mathcal{B} âˆ© L^0(T Ã— Î©)}}]

	\stopsection

	\startsection [title={Step 4: \m{X âˆˆ \mathcal{A} âˆ© L^2(T Ã— Î©)}}]

	\stopsection

	\startsection [title={Step 5: \m{X âˆˆ \mathcal{A} âˆ© \bcrl[X âˆˆ â„‚^{T Ã— Î©} : âˆ€ t â‰¥ 0, âˆ«_0^t X(s, â‹…) \d s < âˆ] a.s.}}]

	\stopsection

	\startsection [title={Properties of the ItÃ´ integral}]

		In what follows, assume the following. Let \m{X, Y âˆˆ \mathcal{A} âˆ© L^2(T Ã— Î©); (X_n), (Y_n) âŠ‚ \mathcal{A} âˆ© \mathcal{S}} such that \m{\norm[X_n - X] â†’ 0} and \m{\norm[Y_n - Y] â†’ 0}. Let \m{z âˆˆ â„‚}.

		\startsubsection [title={Linearity: \m{\norm[z ğ“˜(X) + ğ“˜(Y) - ğ“˜(zX+Y)] = 0}}]
			First, note that \m{\norm[(z X_n + Y_n) - (z X + Y)] â‰¤ \abs[z] \norm[X_n - X] + \norm[Y_n - Y] â†’ 0}. Now, by the linearity of the integral \m{ğ“˜:\mathcal{A} âˆ© \mathcal{S} â†’ L^2(Î©)}, we have
			\startformula \startalign[align={left}]
				\NC  \norm[z ğ“˜(X) + ğ“˜(Y) - ğ“˜(z X + Y)]  \NR
				\NC  =  \norm[z ğ“˜(X) + ğ“˜(Y) - z ğ“˜(X_n) - ğ“˜(Y_n) + ğ“˜(z X_n + Y_n) - ğ“˜(z X + Y)]  \NR
				\NC  â‰¤  \abs[z] \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[ğ“˜(Y) - ğ“˜(Y_n)] + \norm[ğ“˜(z X_n + Y_n) - ğ“˜(z X + Y)]  â†’  0 .
			\stopalign \stopformula
		\stopsubsection

		\startsubsection [title={ItÃ´ isometry: \m{\norm[ğ“˜(X)] = \norm[X]}}]
			Using the isometry of the integral \m{ğ“˜:\mathcal{A} âˆ© \mathcal{S} â†’ L^2(Î©)}, we have
			\startformula \startalign[align={left,left}]
				\NC  \norm[ğ“˜(X)]  \NC  â‰¤  \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[ğ“˜(X_n)]  \NR
				\NC  \NC  =  \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[X_n]  \NR
				\NC  \NC  =  \norm[ğ“˜(X) - ğ“˜(X_n)] + \norm[X_n - X] + \norm[X]  â†’  \norm[X] .
			\stopalign \stopformula
			Note that the \quote{ItÃ´ isometry} is actually a unitary transformation.
		\stopsubsection

		\startsubsection [title={Martingale property: \m{ğ”¼\brnd[ğ“˜_t(X) âˆ£ â„±_s] = ğ“˜_s(X)} a.s.}]
			The martingale property of the integral \m{ğ“˜:\mathcal{A} âˆ© \mathcal{S} â†’ L^2(Î©)} gives \m{ğ”¼\brnd[ğ“˜_t(X_n) - ğ“˜_s(X_n) âˆ£ â„±_s] = 0}. Using this and the unitariness of the ItÃ´ isometry, we get
			\startformula \startalign[align={left,left}]
				\NC  \norm[ğ”¼{\brnd[ğ“˜_t(X) - ğ“˜_s(X) âˆ£ â„±_s]}]^2  \NR
				\NC  =  ğ”¼\abs[ğ”¼{\brnd[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X) âˆ£ â„±_s]} + ğ”¼{\brnd[ğ“˜_t(X_n) - ğ“˜_s(X_n) âˆ£ â„±_s]}]^2  \NR
				\NC  =  ğ”¼\abs[ğ”¼{\brnd[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X) âˆ£ â„±_s]} + 0]^2  \NR
				\NC  â‰¤  ğ”¼ğ”¼\brnd[{\abs[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X)]}^2 âˆ£ â„±_s]  \NR
				\NC  =  ğ”¼\abs[ğ“˜_t(X) - ğ“˜_t(X_n) + ğ“˜_s(X_n) - ğ“˜_s(X)]^2  \NR
				\NC  =  \norm[ğ“˜_t(X - X_n) + ğ“˜_s(X_n - X)]^2  \NR
				\NC  â‰¤  2 \brnd[{\norm[ğ“˜_t(X - X_n)]}^2 + {\norm[ğ“˜_s(X_n - X)]}^2]  \qquad  \bsqr[{\norm[a + b]^2 â‰¤ 2\brnd[{\norm[a]^2 + \norm[b]^2}]}]  \NR
				\NC  =  2 \brnd[{\norm[X - X_n]}^2 + {\norm[X_n - X]}^2]  =  4 \norm[X_n - X]^2  â†’  0 .
			\stopalign \stopformula
		\stopsubsection

	\stopsection

	% In what follows, \m{T = [0, t], \ t âˆˆ [0, âˆ)}, \m{X: T Ã— Î© â†’ â„}, and \m{\mathcal{S}} represent the class of simple processes.

	% \startsection [title={Approximation of \m{\mathcal{S} âˆ‹ X_n â†’ X âˆˆ L^2(T Ã— Î©)}}]
	% 	\startsubsection [title={Step 1: \m{\mathcal{S} âˆ‹ X_n â†’ X âˆˆ C âˆ© B}}]
	% 		Suppose \m{X} is a progressive, adapted, \emph{bounded on \m{I}} \m{t}-a.s., and has \emph{continuous sample paths} \m{t}-a.s. Then there exist a sequence of bounded simple processes \m{(X_n)} such that \m{X_n â†’ X} in \m{L^2(T Ã— Î©)}.

	% 		Proof.

	% 		Let \m{(Î”_n)} be a sequence of progressively finer partitions of \m{I} such that \m{Î”_n = \bcrl[t_0, t_1 â€¦, t_n]; t_0 < t_1 < â€¦ < t_n}, and \m{\norm[Î”_n] â†’ 0} as \m{n â†’ âˆ}. Define the sequence of simple processes \m{X_n(t, Ï‰) = âˆ‘_{j = 0}^{n - 1} X(t_j, Ï‰) ğŸ™_{\intco[t_j, t_{j + 1}]}(t)}. Then, we claim that \m{X_n â†’ X} in \m{L^2(T Ã— Î©)}.

	% 		Firstly fix \m{Ï‰ âˆˆ Î©}. By the \emph{continuity of paths} of \m{X}, \m{âˆ€ Ïµ > 0, âˆƒ Î´ > 0} such that whenever \m{\abs[s - t] < Î´}, we have \m{\abs[X(s, Ï‰) - X(t, Ï‰)] < Ïµ}. Therefore, for \m{\norm[Î”_n] < Î´}, we have
	% 		\startformula
	% 			\abs[X_n(t, Ï‰) - X(t, Ï‰)]
	% 				â‰¤ âˆ‘_{j = 0}^{n - 1} \abs[X(t_j, Ï‰) - X(t, Ï‰)] ğŸ™_{\intco[t_j, t_{j + 1}]}(t)
	% 				< âˆ‘_{j = 0}^{n - 1} Ïµ ğŸ™_{\intco[t_j, t_{j + 1}]}(t)
	% 				= Ïµ .
	% 		\stopformula
	% 		Therefore, \m{âˆ€ t âˆˆ I}, \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)] â†’ 0}, and so \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 â†’ 0}. Moreover, since the sample path \m{X(â‹…, Ï‰)} is \emph{bounded}, we have \m{\abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 â©½ 4 \norm[X(â‹…, Ï‰)]_{L^âˆ(I)}^2 < âˆ}. Therefore by the bounded convergence theorem, \m{âˆ«_0^1 \abs[X_n(t, Ï‰) - X(t, Ï‰)]^2 \d t â†’ 0}.

	% 		Now varying \m{Ï‰} and taking expectation, we get \m{ğ”¼\brnd[âˆ«_0^1 {\abs[X_n(t, Ï‰) - X(t, Ï‰)]}^2 \d t] â†’ 0}. In other words, \m{\norm[X_n - X]_{L^2(T Ã— Î©)} â†’ 0}.
	% 	\stopsubsection
	% 	\startsubsection [title={Step 2: \m{C âˆ© B âˆ‹ X_n â†’ X âˆˆ B}}]

	% 	\stopsubsection
	% 	\startsubsection [title={Step 3: \m{B âˆ‹ X_n â†’ X âˆˆ L^2(T Ã— Î©)}}]

	% 	\stopsubsection
	% \stopsection

	\startsection[title={ItÃ´ formula for multidimensional processes}]

		Recall the discussion of Taylor series for multivariate functionals in section \in[sec:taylor].

		This part is from \cite[SundarKallianpur2014], Â§ 5.3, 5.4 and 5.6.

		A (local, continuous) semimartingale is a process \m{X_t} that can be written as \m{X_t = X_0 + M_t + A_t}, where
		\startitemize [n, joinedup, nowhite, after]
			\item  \m{M_t} is a mean-zero (local, continuous) martingale, and
			\item  \m{A_t} is an right-continuous adapted process of locally bounded variation.
		\stopitemize
		This is equivalently represented in the differential form as \m{\d X_t = \d M_t + \d A_t}.

		Let \m{X_t} be a \m{d}-dimensional semimartingale, and let \m{Y_t = f(X_t)}, where \m{f âˆˆ C^2(â„)}. Then
		\startformula
			\d Y_t
			=  \d f(X_t)
			=  f'(X_t) \d A_t
			 + f'(X_t) \d M_t
			 + \frac12 f''(X_t) \d \inn[M_t, M_t],
		\stopformula
		where we use the rule \m{\d \inn[B^{(j)}, B^{(j)}]_t = \brnd[\d B_t^{(j)}]^2 = \d t}, everything else being 0.

		Alternatively, as from \cite[Kuo2006], we have the following.

		Let \m{X_t= (X_t^{(1)}, \dots, X_t^{(d)})} be a \m{d}-dimensional process and \m{f(t, x)} be a functional of \m{(t, X_t)}. Then the ItÃ´ formula becomes
		\startformula
			\d f(t, X_t)  =  \brnd[{\inn[\d t, \D_t]} f](t, X_t)
				+ \brnd[{\inn[\d x, \D_x]} f](t, X_t)
				+ \frac12 \brnd[{\inn[\d x, \D_x]}^2 f](t, X_t) ,
		\stopformula
		or in short, \m{\d f  =  \brnd[{\inn[\d t, \D_t]}  +  {\inn[\d x, \D_x]}  +  \frac12 {\inn[\d x, \D_x]}^2] f}.

	\stopsection

	\startsection[title={Adapted and instantly independent implies deterministic}]

		Let \m{X_â‹…} be both adapted and instantly independent. Then for any fixed \m{t}, we have \m{X_t = ğ”¼(X_t âˆ£ â„±_t) = ğ”¼ X_t}. Therefore, \m{X_t} is constant w.r.t. \m{Ï‰} for each \m{t}. Therefore \m{X_â‹…} must be deterministic.

	\stopsection

\stopchapter


\startchapter [title={Examples}]

	\startsection [title={Find \m{ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t]^2}.}]

		By Fubini's theorem,
		\startformula
			ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t]^2  \NC  =  ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t âˆ«_0^1 B_s^2 \d s]  =  ğ”¼\brnd[âˆ«_0^1 âˆ«_0^1 B_t^2 B_s^2 \d s \d t]  =  âˆ«_0^1 âˆ«_0^1 ğ”¼(B_t^2 B_s^2) \d s \d t .
		\stopformula
		\startformula \startalign[n=3, align={left, right, left}]
			\NC  \text{Now, } âˆ€s âˆˆ [0, t],  \NC  ğ”¼(B_t^2 B_s^2)  \NC  =  ğ”¼(ğ”¼(B_t^2 B_s^2 âˆ£ â„±_s))  =  ğ”¼(B_s^2 ğ”¼((B_t^2 - t) + t âˆ£ â„±_s))  \NR
			\NC  \NC  \NC  =  ğ”¼(B_s^2 ((B_s^2 - s) + t))  =  ğ”¼(B_s^2 ((B_s^2 - s) + t))  \NR
			\NC  \NC  \NC  =  ğ”¼(B_s^4 - s B_s^2 + t B_s^2)  =  3s^2 - s^2 + ts  =  2s^2 + ts .  \NR
			\NC  \text{So }  \NC  ğ”¼\brnd[âˆ«_0^1 B_t^2 \d t]^2  \NC  =  2 âˆ«_0^1 âˆ«_0^t (2s^2 + ts) \d s \d t  =  \frac{7}{9} .  \NR
		\stopalign \stopformula

	\stopsection

	\startsection [title={Find \m{ğ•\brnd[âˆ«_0^1 t^2 B_t \d t]}.}]

		By Fubini's theorem, \m{ğ”¼\brnd[âˆ«_0^1 t^2 B_t \d t]  =  âˆ«_0^1 t^2 ğ”¼B_t \d t  =  0}. So by Fubini's theorem (again),
		\startformula \startalign[align={left, left}]
			\NC  ğ•\brnd[âˆ«_0^1 t^2 B_t \d t]  \NC  =  ğ”¼\brnd[âˆ«_0^1 t^2 B_t \d t]^2  =  ğ”¼\brnd[âˆ«_0^1 t^2 B_t \d t âˆ«_0^1 s^2 B_s \d s]  \NR
			\NC  \NC  =  ğ”¼\brnd[âˆ«_0^1 âˆ«_0^1 t^2 s^2 B_t B_s \d s \d t]  =  âˆ«_0^1 âˆ«_0^1 t^2 s^2 ğ”¼(B_t B_s) \d s \d t  \NR
			\NC  \NC  =  âˆ«_0^1 âˆ«_0^1 t^2 s^2 (t âˆ§ s) \d s \d t  =  2 âˆ«_0^1 âˆ«_0^t t^2 s^2 s \d s \d t  =  \frac{1}{14} .  \NR
		\stopalign \stopformula

	\stopsection

	\startsection [title={Calculate \m{âˆ«_0^T e^{B_t^2} \d B_t}.}]
		Let \m{f(x) = âˆ«_0^x e^{t^2} \d t}. Then \m{f'(x) = e^{x^2}} and \m{f''(x) = 2 x e^{x^2}}.

		Now, using the ItÃ´ formula, we get \m{\d \brnd[âˆ«_0^x e^{t^2} \d t] = e^{B_t^2} \d B_t + B_t e^{B_t^2} \d t}, which gives us \m{âˆ«_0^T e^{B_t^2} \d B_t =  âˆ«_0^{B_T} e^{t^2} \d t - âˆ«_0^T B_t e^{B_t^2} \d t}.
	\stopsection

\stopchapter

\stopcomponent
