\startcomponent *

\product  prd-analysis

\startchapter[title={Elementary ideas}]

	\startsection [title={\m{σ}-algebras}]
		\startitemize [1, nowhite, after]
			\item  \m{𝓘 ⊂ 𝓑 ⊂ \overline{𝓑} = 𝓛 ⊂ 2^ℝ}    % TODO: Add counterexamples
			\item  \m{\abs[𝓘] = \abs[𝓑] = \abs[ℝ] = ℵ_{1}, \ \abs[\overline{𝓑}] = \abs[2^ℝ] = ℵ_{2}}
		\stopitemize
	\stopsection

	\startsection [title={Examples}]
		\startitemize [1, nowhite, after]
			\item  \m{\brnd[x ↦ \frac{1}{\sqrt{x}} 𝟙_{[0, 1]}(x)] ∈ L^1 ∖ L^2}
			\item  \m{\brnd[x ↦ \frac{1}{x} 𝟙_{[1, ∞)}(x)] ∈ L^2 ∖ L^1}
		\stopitemize
	\stopsection

	\startsection [title={Measurability of \m{\inf, \sup, \liminf, \limsup} TODO}]
		Let \m{X_n} be a discrete-time stochastic process.
		Then \m{\bcrl[\inf X_t ≥ c]  =  ⋂_t \bcrl[X_t ≥ c]  }
	\stopsection

	\startsection [title={Method of substitution}]
		See Folland - Real Analysis Theorem (2.43).
	\stopsection

\stopchapter


\startchapter [title={Borel--Cantelli lemmas}]

	\starttheorem[title={Borel--Cantelli 1}]
		Let \m{(E_n) ⊂ ℱ} such that \m{∑ℙ(E_n) < ∞}. Then \m{ℙ\brnd[E_n \infinitelyoften] = 0}.
	\stoptheorem
	\startproof
		Since \m{∑ℙ(E_n) < ∞}, for any fixed \m{n ∈ ℕ}, we have
		\startformula
			ℙ\brnd[E_n \infinitelyoften]  =  ℙ\brnd[⋂_{n} ⋃_{m ≥ n} E_m]  ≤  ℙ\brnd[⋃_{m ≥ n} E_m]  ≤  ∑_{m ≥ n} ℙ(E_n)  →  0 .
		\stopformula
	\stopproof

	Counterexample of the converse of BC1: Take \m{((0, 1], λ)} as the probability space, and \m{E_n = \brnd[0, \frac{1}{n^2}]}. Then \m{ℙ\brnd[E_n \infinitelyoften] = 1}, but \m{∑ℙ(E_n) < ∞}.

	\starttheorem[title={Borel--Cantelli 2}]
		Let \m{(E_n) ⊂ ℱ} be (mutually) independent such that \m{∑ℙ(E_n) = ∞}. Then \m{ℙ\brnd[E_n \infinitelyoften] = 1}.
	\stoptheorem
	\startproof
		Since \m{ℙ\brnd[{\brnd[E_n \infinitelyoften]}^∁] = ℙ\brnd[E_n^∁ \eventually]}, it is equivalent to prove that \m{ℙ\brnd[E_n^∁ \eventually] = 0}. Using independence and the fact that \m{1 - x < e^{-x}}, for each fixed \m{n ∈ ℕ}, we have
		\startformula
			ℙ\brnd[⋂_{m = n}^N E_n^∁]  =  ∏_{m = n}^N ℙ\brnd[E_n^∁]  =  ∏_{m = n}^N \brnd[1 - ℙ(E_n)]  ≤  ∏_{m = n}^N e^{- ℙ(E_n)}  =  e^{- ∑_{m = n}^N ℙ(E_n)} .
		\stopformula
		Taking \m{N → ∞}, we get \m{ℙ\brnd[⋂_{m ≥ n} E_n^∁] → 0}. Therefore
		\startformula
			ℙ\brnd[E_n^∁ \eventually]  =  ℙ\brnd[⋃_n ⋂_{m ≥ n} E_n^∁]   ≤  ∑_n ℙ\brnd[⋂_{m = n}^N E_n^∁]  =  0 .
		\stopformula
	\stopproof

\stopchapter


\startchapter [title={Modes of convergence}]

	Study this part from \goto{Robert L Wolpert - Convergence in \m{ℝ^d} and in metric spaces}[url(https://www2.stat.duke.edu/courses/Fall18/sta711/lec/wk-07.pdf)].
	% ToDo: Correct this diagram.
	In this diagram, the top row represents \quote{\emph{point independent}} modes of convergence and the bottom row represents the \quote{\emph{point dependent}} modes of convergence.

	\blank[line]

	\starttikzcd [row sep=huge, column sep=huge]
		unif
				\arrow[r, middlegreen]
				\arrow[d, middlegreen]
			\NC  L^∞
				\arrow[r, middlegreen]
				\arrow[d, middlegreen]
			\NC  L^2
				\arrow[r, middlegreen]
				\arrow[dl, slategray, swap, "(X_n ⟂) ∧ (𝔼 X_n = 0) ∧ (∑ 𝕍 X_n < ∞)" description]
			\NC  L^1
				\arrow[r, middlegreen]
			\NC  ℙ
				\arrow[r, middlegreen]
				\arrow[dlll, bend left, magenta, "∀ \text{ subseq } ∃ \text{ subsubseq }" description]
				\arrow[l, bend right, darkblue, swap, "\text{u.i. ∨ unif bound}"]
			\NC  w^*
				\arrow[l, bend right, swap, darkorange, "const" description]
		\NR  pt
				\arrow[r, middlegreen]
			\NC  a.s.
				\arrow[urrr, middlegreen]
				\arrow[urr, dotted, red, near end, "LDCT?" description]  \NR
	\stoptikzcd

	Legend
	\startitemize [1, nowhite, after]
		\item  \color[middlegreen]{green: automatic implication}
		\item  any other color: depends on the mentioned condition
	\stopitemize

\stopchapter


\startchapter [title={Conditioning}, reference=sec:conditioning]

	\startsection[title={In \m{L^2}, conditional expectation is a projection}]
		Let \m{(Ω, ℱ, ℙ)} be a probability space, \m{𝓖 ⊆ ℱ} be a sub-σ-algebra, and \m{X ∈ L^2(ℱ)} be a random variable. Let \m{π_{L^2(𝓖)}} denote the projection operator onto \m{L^2(𝓖)}. We know that projection operators are self-adjoint. So \m{∀ E ∈ 𝓖},
		\startformula \startalign[n=3]
			\NC  ∫_E 𝔼(X ∣ 𝓖) \d ℙ \big|_𝓖  \NC =  ∫_E X \d ℙ  =  ∫ 𝟙_E X \d ℙ  =  \inn[𝟙_E, X]  \NC  [\text{definitions}]  \NR
			\NC  \NC =  \inn[π_{L^2(𝓖)} 𝟙_E, X]  \NC  [E ∈ 𝓖 ⟹ 𝟙_E ∈ L^2(𝓖)]  \NR
			\NC  \NC =  \inn[𝟙_E, π_{L^2(𝓖)}^* X]  =  \inn[𝟙_E, π_{L^2(𝓖)} X]  \NC  [\text{self-adjointness}]  \NR
			\NC  \NC =  ∫ 𝟙_E π_{L^2(𝓖)} X \d ℙ \big|_𝓖  =  ∫_E π_{L^2(𝓖)} X \d ℙ \big|_𝓖 .  \qquad  \NC  [\text{definitions}]  \NR
		\stopalign \stopformula
		Therefore, \m{𝔼(X ∣ 𝓖) = π_{L^2(𝓖)} X} a.s.

	\stopsection

	\startsection [title={Uncorrelated does not imply independence}]
		See the wikipedia entries on \goto{uncorrelated random variables}[url(https://en.wikipedia.org/wiki/Uncorrelated_random_variables)] and \goto{normally distributed and uncorrelated does not imply independent}[url(https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent)].
	\stopsection

	\startsection [title={\m{ϕ_{aX + bY} = ϕ_{aX} ϕ_{bY} \ ∀ (a, b) ∈ ℝ^2} implies independence}]
		See \goto{MathSx:1802289}[url(https://math.stackexchange.com/questions/1802289)].
	\stopsection

\stopchapter

\stopcomponent
